{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d36ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4b13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'C:\\Users\\User\\Documents\\Atividades_andamento\\AI_Prompt_Eng\\data'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b66bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values # used to save passawords credicard numbers,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c66c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de114fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959a58a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis prompt\n",
    "#  here you can see how to perform sentiment analysis \n",
    "#  you need to start with say you want a sentiment analysis to get it... just this\n",
    "sentiment ='''\n",
    "It's ok. They need to understand that anyone who likes oriental food \n",
    "is not willing to eat kilos of cream cheese (and even low quality!) in every part of the company.\n",
    "'''\n",
    "prompt = f'''classify following text sentiment as positive, neutral, negative\n",
    "Input: \n",
    "{sentiment}\n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe20e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemical engineers are responsible for translating lab processes into practical applications for commercial production and constantly strive to improve these processes. They depend on a strong foundation in math, physics, and chemistry, with biology also becoming more important in their field.\n"
     ]
    }
   ],
   "source": [
    "# summarization  prompt\n",
    "#\n",
    "text = '''\n",
    "Chemical engineers translate processes developed in the lab into practical applications for the commercial\n",
    "production of products, and then work to maintain and improve those processes. They rely on the main foundations\n",
    "of engineering: math, physics, and chemistry. Biology also plays an increasingly important role.\n",
    "'''\n",
    "prompt = f'''\n",
    "summarize the following text:\n",
    "desired format: 2 to 3 sentences\n",
    "Input:{text}\n",
    "summary:\n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d5e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"edamame\",\n",
      "    \"sushi\",\n",
      "    \"salad\",\n",
      "    \"sashimi\",\n",
      "    \"chicken\",\n",
      "    \"beef cheeks\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# data extraction  prompt\n",
    "#\n",
    "text = '''\n",
    "We started with edamame and sushi. I opted for a salad and sashimi. \n",
    "The salad was baby kale with a green goddess vinaigrette dressing which was very tasty. \n",
    "My hubby and sister-in-law had their variation of Cesar and both loved it. \n",
    "While I enjoyed sashimi, then enjoyed chicken (sis) and beef cheeks (hubby).\n",
    "'''\n",
    "prompt = f'''\n",
    "extract food itens from the following text\n",
    "desired format: json array\n",
    "Input:\n",
    "{text}\n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d5e52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis prompt\n",
    "# one shot prompt\n",
    "#\n",
    "sentiment ='''\n",
    "It's ok. They need to understand that anyone who likes oriental food \n",
    "is not willing to eat kilos of cream cheese (and even low quality!) in every part of the company.\n",
    "'''\n",
    "prompt = f'''\n",
    "  classify following text sentiment as positive, neutral, negative\n",
    "  desidered format: -1 (negative), 0 (neutral), 1 (positive)\n",
    "Input:\n",
    "  I love this food\n",
    "Output: 1\n",
    "Input: \n",
    "{sentiment}\n",
    "Output: \n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d2c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Erica is 30 years old, then Beth is 30 - 5 = 25 years old.\n",
      "And if Beth is 25 years old, then Alice is 25 + 7 = 32 years old.\n",
      "Therefore, the age difference between Alice and Erica is 32 - 30 = 2 years.\n"
     ]
    }
   ],
   "source": [
    "# GPT getting wrond answer but using\n",
    "#    let's think step by step it improve the answer\n",
    "#    here without using 'let's think step by step':\n",
    "#    PS: this case there if you run more than one time you can find wrigth answer in some runnings\n",
    "\n",
    "prompt = '''\n",
    "Alice is seven years old than Beth, who is five years old than Erica.\n",
    "What the difference of age betweeen Alice and Erica if Erica is 30 years old?\n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb5b5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Calculate Beth's age\n",
      "Since Beth is 5 years older than Erica, and Erica is 30 years old, Beth's age is 30 + 5 = 35 years old.\n",
      "\n",
      "Step 2: Calculate Alice's age\n",
      "Since Alice is 7 years older than Beth, Alice's age is 35 + 7 = 42 years old.\n",
      "\n",
      "Step 3: Calculate the age difference between Alice and Erica\n",
      "The age difference between Alice and Erica is 42 -\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "Alice is seven years old than Beth, who is five years old than Erica.\n",
    "What the difference of age betweeen Alice and Erica if Erica is 30 years old?\n",
    "Let's think step by step\n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb793117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. If Erica is 30 years old, then Beth is 35 years old (since Beth is 5 years older than Erica).\n",
      "2. If Beth is 35 years old, then Alice is 42 years old (since Alice is 7 years older than Beth).\n",
      "3. The age difference between Alice and Erica is 42 - 30 = 12 years. \n",
      "\n",
      "Therefore, the age difference between Alice and Erica is 12 years.\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "Alice is seven years old than Beth, who is five years old than Erica.\n",
    "What the difference of age betweeen Alice and Erica if Erica is 30 years old?\n",
    "Let's think step by step\n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=200,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ab30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the paper:\n",
    "#     Large Language Models are Zero-Shot Reasoners\n",
    "#     36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n",
    "#     https://arxiv.org/pdf/2205.11916.pdf\n",
    "# Abstract (part:)\n",
    "# While these successes are often attributed to LLMs’\n",
    "# ability for few-shot learning, we show that LLMs are decent zero-shot reasoners\n",
    "# by simply adding “Let’s think step by step” before each answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e039259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Spanish\": \"Morado no es mi color favorito\",\n",
      "  \"French\": \"Le violet n'est pas ma couleur préférée\",\n",
      "  \"Japanese\": \"紫は私のお気に入りの色ではありません\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# example of transform text\n",
    "prompt='''\n",
    "Translate the following text in Spanish, French and Japanese\n",
    "The output should be a json object\n",
    "Input:\n",
    "  Purple it is not my favourite color\n",
    "Output:\n",
    "'''\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=150,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857d046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

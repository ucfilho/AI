{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d36ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4b13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'C:\\Users\\User\\Documents\\Atividades_andamento\\AI_Prompt_Eng\\data'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b66bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values # used to save passawords credicard numbers,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c66c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de114fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9f9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959a58a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis prompt\n",
    "#  here you can see how to perform sentiment analysis \n",
    "#  you need to start with say you want a sentiment analysis to get it... just this\n",
    "part1 = 'classify following text sentiment as positive, neutral, negative' \n",
    "part2 = 'Input:'\n",
    "review = \"It's ok. They need to understand that anyone who likes oriental food is not willing to eat kilos of cream cheese (and even low quality!) in every part of the company.\"\n",
    "question = part1+part2+review\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe20e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemical engineers use principles of math, physics, chemistry, and biology to turn laboratory processes into commercial products. They work to maintain and enhance these processes for improved efficiency and quality.\n"
     ]
    }
   ],
   "source": [
    "# summarization  prompt\n",
    "#\n",
    "part1 = 'summarize the following text'\n",
    "part2='desired format: 2 to 3 sentences'\n",
    "part3 = 'Input:'\n",
    "text1 = 'Chemical engineers translate processes developed in the lab into practical applications for the commercial'\n",
    "text2= 'production of products, and then work to maintain and improve those processes. They rely on the main foundations'\n",
    "text3 = 'of engineering: math, physics, and chemistry. Biology also plays an increasingly important role.' \n",
    "text = text1+text2+text3 +'summary:'\n",
    "question = part1+part2+part3+text\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d5e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"edamame\",\n",
      "  \"sushi\",\n",
      "  \"salad\",\n",
      "  \"sashimi\",\n",
      "  \"baby kale\",\n",
      "  \"green goddess vinaigrette dressing\",\n",
      "  \"Cesar salad\",\n",
      "  \"chicken\",\n",
      "  \"beef cheeks\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# data extraction  prompt\n",
    "#\n",
    "part1 = 'extract food itens from the following text'\n",
    "part2='desired format: json array'\n",
    "part3 = 'Input:'\n",
    "text1 = 'We started with edamame and sushi. I opted for a salad and sashimi. '\n",
    "text2= 'The salad was baby kale with a green goddess vinaigrette dressing which was very tasty. '\n",
    "text3 = 'My hubby and sister-in-law had their variation of Cesar and both loved it. ' \n",
    "text4 ='While I enjoyed sashimi, then enjoyed chicken (sis) and beef cheeks (hubby).'\n",
    "text = text1+text2+text3 +text4+'output:'\n",
    "question = part1+part2+part3+text\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6d5e52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis prompt\n",
    "# one shot prompt\n",
    "#\n",
    "part1 = 'classify following text sentiment as positive, neutral, negative' \n",
    "part2 = 'desidered format: -1 (negative), 0 (neutral), 1 (positive)'\n",
    "part3 = 'Input:'\n",
    "part4 = ' I love this food'\n",
    "part5 = 'Output: 1'\n",
    "review = \"It's ok. They need to understand that anyone who likes oriental food is not willing to eat kilos of cream cheese (and even low quality!) in every part of the company.\"\n",
    "question = part1+part2+part3+part4+part5+ review + 'Output:'\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d2c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Erica is 30 years old, then Beth is 5 years older, making Beth 35 years old. And since Alice is 7 years older than Beth, Alice would be 35 + 7 = 42 years old.\n",
      "\n",
      "Therefore, the age difference between Alice and Erica would be:\n",
      "\n",
      "42 - 30 = 12 years\n"
     ]
    }
   ],
   "source": [
    "# GPT getting wrond answer but using\n",
    "#    let's think step by step it improve the answer\n",
    "#    here without using 'let's think step by step':\n",
    "#    PS: this case there is no difference using or not\n",
    "text1='Alice is seven years old than Beth, who is five years old than Erica.'\n",
    "text2='What the difference of age betweeen Alice and Erica if Erica is 30 years old?'\n",
    "question = text1+text2\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb5b5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Let's assign variables to the ages of each person:\n",
      "- Let A represent Alice's age\n",
      "- Let B represent Beth's age\n",
      "- Let E represent Erica's age\n",
      "\n",
      "2. We know that Alice is seven years older than Beth, so we can write:\n",
      "A = B + 7\n",
      "\n",
      "3. We also know that Beth is five years older than Erica, so we can write:\n",
      "B = E + 5\n",
      "\n",
      "4. Given that Erica is 30 years old, we\n"
     ]
    }
   ],
   "source": [
    "text1='Alice is seven years old than Beth, who is five years old than Erica.'\n",
    "text2='What the difference of age betweeen Alice and Erica if Erica is 30 years old?'\n",
    "text3= \"let's think step by step\"\n",
    "question = text1+text2+text3\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7942bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8wbJvqrqeeCsMx4tb1wANVSkYI6Au', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"1. Let's assign variables to the ages of each person:\\n- Let A represent Alice's age\\n- Let B represent Beth's age\\n- Let E represent Erica's age\\n\\n2. We know that Alice is seven years older than Beth, so we can write:\\nA = B + 7\\n\\n3. We also know that Beth is five years older than Erica, so we can write:\\nB = E + 5\\n\\n4. Given that Erica is 30 years old, we\", role='assistant', function_call=None, tool_calls=None))], created=1708977175, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_86156a94a0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=47, total_tokens=147))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb793117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. We know that Erica is 30 years old. \n",
      "\n",
      "2. Beth is 5 years older than Erica, so Beth is 30 + 5 = 35 years old. \n",
      "\n",
      "3. Alice is 7 years older than Beth, so Alice is 35 + 7 = 42 years old. \n",
      "\n",
      "4. The age difference between Alice and Erica is 42 - 30 = 12 years. \n",
      "\n",
      "Therefore, the age difference between Alice and Erica is 12 years.\n"
     ]
    }
   ],
   "source": [
    "text1='Alice is seven years old than Beth, who is five years old than Erica.'\n",
    "text2='What the difference of age betweeen Alice and Erica if Erica is 30 years old?'\n",
    "text3= \"let's think step by step\"\n",
    "question = text1+text2+text3\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=150,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ab30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the paper:\n",
    "#     Large Language Models are Zero-Shot Reasoners\n",
    "#     36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n",
    "#     https://arxiv.org/pdf/2205.11916.pdf\n",
    "# Abstract (part:)\n",
    "# While these successes are often attributed to LLMs’\n",
    "# ability for few-shot learning, we show that LLMs are decent zero-shot reasoners\n",
    "# by simply adding “Let’s think step by step” before each answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e039259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Spanish\": \"El morado no es mi color favorito\",\n",
      "  \"French\": \"Le violet n'est pas ma couleur préférée\",\n",
      "  \"Japanese\": \"紫は私の好きな色ではありません\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# example of transform text\n",
    "\n",
    "text1='Translate the following text in Spanish, French and Japanese'\n",
    "text2='The output should be a json object'\n",
    "text3= \"Purple it is not my favourite color\"\n",
    "question = text1+text2+text3\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    max_tokens=150,\n",
    "    n=1,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857d046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
